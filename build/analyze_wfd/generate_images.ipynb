{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from build.config import STORAGE_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_format(num):\n",
    "    num = float('{:.3g}'.format(num))\n",
    "    magnitude = 0\n",
    "    if num < 10000:\n",
    "        return str(int(num))\n",
    "    while abs(num) >= 1000:\n",
    "        magnitude += 1\n",
    "        num /= 1000.0\n",
    "    return '{}{}'.format('{:f}'.format(num).rstrip('0').rstrip('.'), ['', 'K', 'M', 'B', 'T'][magnitude])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikidata Dump Index Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "with open(osp.join(STORAGE_FOLDER, 'wikidata_dumps_index.json')) as f:\n",
    "    idx = json.load(f)\n",
    "import pandas as pd\n",
    "\n",
    "df = [(k[:4] + '-' + k[4:6] + '-' + k[-2:],'Wikidata Repository' if v.startswith('https://dumps') else 'Internet Archive') for k,v in idx.items()]\n",
    "df = pd.DataFrame(df, columns=['Date', 'Source'])\n",
    "df['Date'] = df['Date'].astype('datetime64[ns]')\n",
    "\n",
    "plt.figure(dpi=400, figsize=(8,2.5))\n",
    "\n",
    "sns.histplot(df, x='Date', hue='Source', bins=50)\n",
    "plt.savefig('wikidata_dump_index.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type distribution Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build.utils.wd import get_info_wikidata\n",
    "import json\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "def get_stats(version : str, top_k = 20, post : bool = False):\n",
    "    post_str = '' if not post else 'post_'\n",
    "    if not post:\n",
    "        j_dict = json.load(open(osp.join(STORAGE_FOLDER, 'resources/script_stats/process_json_dump_wikidata_%s_json.json' % version)))\n",
    "        n_entities = j_dict['n_entities']\n",
    "        type_count = list(j_dict['type_count'].items())\n",
    "    else:\n",
    "        j_dict = json.load(open(osp.join(STORAGE_FOLDER,'resources/script_stats/preprocess_dump_%s.json' % version)))\n",
    "        n_entities = j_dict['post_n_wiki_entities']\n",
    "        type_count = list(j_dict['%stype_count' % post_str].items())\n",
    "    \n",
    "    # Temporary solution\n",
    "    type_count = [(k,v) for k,v in type_count if k != \"none\"]\n",
    "    top_k_type_count = sorted(type_count, key=lambda x : x[1], reverse=True)\n",
    "\n",
    "    others_count = sum([x[1] for x in top_k_type_count[top_k:]])\n",
    "    top_k_type_count = top_k_type_count[:top_k]\n",
    "    top_k_type_count.append(('others', others_count))\n",
    "    type2label = {k:v.get('name', k) for k,v in get_info_wikidata([x[0] for x in top_k_type_count], version=version).items()}\n",
    "    stats = [(type2label[x] if x != 'others' else 'others', y) for x,y in top_k_type_count]\n",
    "    # stats = {k : v/len(types) for k,v in Counter(types).items()}\n",
    "    stats = pd.DataFrame(data=stats, columns=['Type', 'Count'])\n",
    "    return stats, n_entities\n",
    "\n",
    "top_k = 6\n",
    "new_stats, pre_n_entities_new = get_stats('new', top_k=top_k)\n",
    "old_stats, pre_n_entities_old = get_stats('old', top_k=top_k)\n",
    "post_new_stats, post_n_entities_new = get_stats('new', post=True, top_k=top_k)\n",
    "post_old_stats, post_n_entities_old = get_stats('old', post=True, top_k=top_k)\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}], [{\"type\": \"pie\"}, {\"type\": \"pie\"}]])\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(values=old_stats['Count'], labels=old_stats['Type'], title='Old Wikidata before preprocessing<br>(%s entities)' % human_format(pre_n_entities_old), \n",
    "           domain=dict(x=[0, 0.5], y=[0, 0.5]), name='haha'),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Pie(values=post_old_stats['Count'], labels=post_old_stats['Type'], title='Old Wikidata after preprocessing<br>(%s entities)' % human_format(post_n_entities_old),\n",
    "           domain=dict(x=[0.5, 1], y=[0, 0.5])),\n",
    "    row=1, col=2\n",
    "    \n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Pie(values=new_stats['Count'], labels=new_stats['Type'], title='New Wikidata before preprocessing<br>(%s entities)' % human_format(pre_n_entities_new), \n",
    "           domain=dict(x=[0, 0.5], y=[0.5, 1])),\n",
    "    row=2, col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Pie(values=post_new_stats['Count'], labels=post_new_stats['Type'], title='New Wikidata after preprocessing<br>(%s entities)' % human_format(post_n_entities_new), \n",
    "           domain=dict(x=[0.5, 1], y=[0.5, 1])),\n",
    "    row=2, col=2,\n",
    ")\n",
    "fig.update_layout(\n",
    "    margin=dict(l=5, r=5, t=5, b=5),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image('./wikidata_type_distribution.png', scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([new_stats, post_new_stats], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most popular relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build.utils.wd import get_info_wikidata\n",
    "import json\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "def get_stats(version : str, top_k = 20, post : bool = False):\n",
    "    post_str = '' if not post else 'post_'\n",
    "    if not post:\n",
    "        j_dict = json.load(open(osp.join(STORAGE_FOLDER, 'resources/script_stats/process_json_dump_wikidata_%s_json.json' % version)))\n",
    "        n_relations = len(j_dict['relation_count'])\n",
    "        relation_count = list(j_dict['relation_count'].items())\n",
    "    else:\n",
    "        j_dict = json.load(open(osp.join(STORAGE_FOLDER, 'resources/script_stats/preprocess_dump_%s.json' % version)))\n",
    "        n_relations = len(j_dict['post_relation_count'])\n",
    "        relation_count = list(j_dict['%srelation_count' % post_str].items())\n",
    "    \n",
    "    # Temporary solution\n",
    "    relation_count = [(k,v) for k,v in relation_count]\n",
    "    top_k_relation_count = sorted(relation_count, key=lambda x : x[1], reverse=True)\n",
    "\n",
    "    others_count = sum([x[1] for x in top_k_relation_count[top_k:]])\n",
    "    top_k_relation_count = top_k_relation_count[:top_k]\n",
    "    # top_k_relation_count.append(('others', others_count))\n",
    "    type2label = {k:v.get('name', k) for k,v in get_info_wikidata([x[0] for x in top_k_relation_count], version=version).items()}\n",
    "    stats = [(type2label[x] if x != 'others' else 'others', y) for x,y in top_k_relation_count]\n",
    "    # stats = {k : v/len(types) for k,v in Counter(types).items()}\n",
    "    stats = pd.DataFrame(data=stats, columns=['Relation', 'Count'])\n",
    "    stats['Relation'] = stats['Relation'].apply(lambda x : x[:20] + ('...' if len(x) > 20 else ''))\n",
    "    return stats, n_relations\n",
    "\n",
    "top_k = 15\n",
    "new_stats, pre_n_relations_new = get_stats('new', top_k=top_k)\n",
    "old_stats, pre_n_relations_old = get_stats('old', top_k=top_k)\n",
    "post_new_stats, post_n_relations_new = get_stats('new', post=True, top_k=top_k)\n",
    "post_old_stats, post_n_relations_old = get_stats('old', post=True, top_k=top_k)\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2,\n",
    "                    subplot_titles=(\n",
    "                        'Old Wikidata before preprocessing<br>(%s relations)' % human_format(pre_n_relations_old),\n",
    "                        'Old Wikidata after preprocessing<br>(%s relations)' % human_format(post_n_relations_old),\n",
    "                        'New Wikidata before preprocessing<br>(%s relations)' % human_format(pre_n_relations_new), \n",
    "                        'New Wikidata after preprocessing<br>(%s relations)' % human_format(post_n_relations_new), \n",
    "                        )\n",
    "                        )\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(y=old_stats['Count'], x=old_stats['Relation'], marker_color='#636EFA'),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(y=post_old_stats['Count'], x=post_old_stats['Relation'], marker_color='#636EFA'),\n",
    "    row=1, col=2\n",
    "    \n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(y=new_stats['Count'], x=new_stats['Relation'], marker_color='#00CC96'),\n",
    "    row=2, col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(y=post_new_stats['Count'], x=post_new_stats['Relation'], marker_color='#00CC96'),\n",
    "    row=2, col=2,\n",
    ")\n",
    "fig.update_layout(\n",
    "    margin=dict(l=5, r=5, t=50, b=5),\n",
    ")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()\n",
    "fig.write_image('./wikidata_relation_distribution.png', scale=4, width=1500, height=630)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering process visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IMPORTANT : install graphviz for this cell to work using this command \"sudo apt install graphviz\"\n",
    "\n",
    "import json\n",
    "from build.utils.wd import db\n",
    "from tree_plot import tree_plot\n",
    "\n",
    "def generate_filtering_process_image(version : str):\n",
    "    # Loading stats\n",
    "    stats = json.load(open(osp.join(STORAGE_FOLDER,'resources/script_stats/preprocess_dump_%s.json' % version)))\n",
    "    stats0 = json.load(open(osp.join(STORAGE_FOLDER,'resources/script_stats/process_json_dump_wikidata_%s_json.json' % version)))\n",
    "\n",
    "    n_entities0 = stats0['n_entities']\n",
    "    n_groups0 = stats0['n_groups']\n",
    "    n_triples0 = stats0['n_triples']\n",
    "    n_relations0 = len(stats0['relation_count'])\n",
    "    n_triples_banned_types = stats0['n_triples_banned_types']\n",
    "    n_triples_rank_deprecated = stats0['n_triples_deprecated']\n",
    "    n_triples = stats['n_triples']\n",
    "    n_irrelevant_entities = stats['n_irrelevant_entities']\n",
    "    n_irrelevant_entities_triples = stats['n_irrelevant_entities_triples']\n",
    "    n_entities = db['wikidata_new_json'].estimated_document_count()\n",
    "    n_novalue_triples = stats['n_novalue']\n",
    "    n_somevalue_triples = stats['n_somevalue']\n",
    "    n_corrections_group = stats['n_corrections']\n",
    "    n_corrections_triples = stats['n_corrections']\n",
    "    n_empty_entities = stats['n_empty_entities']\n",
    "    n_temp_rel_pit_deprecation_triples = stats['n_temp_rel_pit_deprecation_all']\n",
    "    n_temp_rel_pit_deprecation_groups = stats['n_temp_rel_pit_deprecation']\n",
    "    n_triples_filtered_because_of_meta = stats['n_triples_filtered_because_of_meta']\n",
    "    n_groups_filtered_because_of_meta = sum(stats['meta_relations_count'].values())\n",
    "    n_restricted_triples = stats['n_restricted_triples']\n",
    "    n_wiki_entities = stats['n_wiki_entities']\n",
    "    n_relations = len(stats['relation_count'])\n",
    "    n_groups = sum(stats['relation_count'].values())\n",
    "    n_relevant_entities = stats['n_relevant_entities']\n",
    "    n_relevant_triples = stats['n_relevant_entities_triples']\n",
    "    n_relevant_groups = stats['n_relevant_entities_groups']\n",
    "\n",
    "    n_group_post_deprecated_banned = stats0['n_groups_post']\n",
    "\n",
    "    # Defining each node of the tree\n",
    "\n",
    "    state0 = \"#B#Entities = %s\\nRelations = %s\\nGroups = %s\\nTriples = %s\" % (human_format(n_entities0), human_format(n_relations0), human_format(n_groups0), human_format(n_triples0))\n",
    "\n",
    "    state_banned_deprecated = '#B#Entities = %s\\nGroups = %s\\nTriples = %s' % (human_format(n_entities0), human_format(n_group_post_deprecated_banned), human_format(n_triples0-n_triples_banned_types-n_triples_rank_deprecated))\n",
    "    how_many_banned_deprecated = '<R<<<u>Remove banned types<br></br>and rank deprecated</u><br></br>Groups = %s (-%.1f%%)<br></br>Triples = %s (-%.1f%%)>' % (human_format(n_groups0-n_group_post_deprecated_banned), (n_groups0-n_group_post_deprecated_banned)/n_groups0*100, human_format(n_triples_banned_types+n_triples_rank_deprecated), (n_triples_banned_types+n_triples_rank_deprecated)/n_triples0*100)\n",
    "\n",
    "    state_relevant_wikipedia = '#B#Entities = %s\\nGroups = %s\\nTriples = %s' % (human_format(n_wiki_entities-n_irrelevant_entities), human_format(n_relevant_groups), human_format(n_triples-n_irrelevant_entities_triples))\n",
    "    how_many_irrelevant_wikipedia = '<R<<<u>Keep only relevant<br></br>Wikipedia entities</u><br></br>Entities = %s (-%.1f%%)<br></br>Groups = %s (-%.1f%%)<br></br>Triples = %s (-%.1f%%)>' % (human_format(n_entities0-(n_wiki_entities-n_irrelevant_entities)), (n_entities0-(n_wiki_entities-n_irrelevant_entities))/n_entities0*100,human_format(n_group_post_deprecated_banned-n_relevant_groups), (n_group_post_deprecated_banned-n_relevant_groups)/ n_group_post_deprecated_banned*100, human_format(n_triples0-n_triples_banned_types-n_triples_rank_deprecated-(n_triples-n_irrelevant_entities_triples)), (n_triples0-n_triples_banned_types-n_triples_rank_deprecated-(n_triples-n_irrelevant_entities_triples))/(n_triples0-n_triples_banned_types-n_triples_rank_deprecated)*100)\n",
    "\n",
    "    state_after_meta_removal = '#B#Entities = %s\\nGroups = %s\\nTriples = %s' % (human_format(n_relevant_entities), human_format(n_relevant_groups-n_groups_filtered_because_of_meta), human_format(n_relevant_triples-n_triples_filtered_because_of_meta))\n",
    "    how_many_meta = '<R<<<u>Remove meta triples</u><br></br>Groups = %s (-%.1f%%)<br></br>Triples = %s (-%.1f%%)>' % (human_format(n_groups_filtered_because_of_meta), n_groups_filtered_because_of_meta/n_relevant_groups*100, human_format(n_groups_filtered_because_of_meta), n_triples_filtered_because_of_meta/n_relevant_triples*100)\n",
    "\n",
    "    n_triples_after_meta = n_relevant_triples-n_triples_filtered_because_of_meta\n",
    "    n_groups_after_meta = n_relevant_groups-n_groups_filtered_because_of_meta\n",
    "    n_novalue_somevalue_group = stats['n_novalue_somevalue_group']\n",
    "\n",
    "    state_after_special_value = '#B#Entities = %s\\nGroups = %s\\nTriples = %s' % (human_format(n_relevant_entities), human_format(n_groups_after_meta-n_novalue_somevalue_group), human_format(n_triples_after_meta- (n_novalue_triples + n_somevalue_triples)))\n",
    "    how_many_special = '<R<<<u>Remove special values</u><br></br>Groups = %s (-%.1f%%)<br></br>Triples = %s (-%.1f%%)>' % (human_format(n_novalue_somevalue_group), n_novalue_somevalue_group/n_groups_after_meta*100, human_format(n_novalue_triples + n_somevalue_triples), (n_novalue_triples + n_somevalue_triples)/n_triples_after_meta*100)\n",
    "\n",
    "    n_triples_after_special = n_triples_after_meta - (n_novalue_triples + n_somevalue_triples)\n",
    "    n_groups_after_special = n_groups_after_meta-n_novalue_somevalue_group\n",
    "    n_restricted_group_removed = stats['n_restricted_group_removed']\n",
    "\n",
    "    state_after_restricted_removal = '#B#Entities = %s\\nGroups = %s\\nTriples = %s' % (human_format(n_relevant_entities), human_format(n_groups_after_special-n_restricted_group_removed), human_format(n_triples_after_special- n_restricted_triples))\n",
    "    how_many_restricted = '<R<<<u>Remove restricted triples</u><br></br>Groups = %s (-%.1f%%)<br></br>Triples = %s (-%.1f%%)>' % (human_format(n_restricted_group_removed), (n_restricted_group_removed)/n_groups_after_special*100, human_format(n_restricted_triples), n_restricted_triples/n_triples_after_special*100)\n",
    "\n",
    "    n_triples_after_restricted = n_triples_after_special - n_restricted_triples\n",
    "    n_groups_after_restricted = n_groups_after_special-n_restricted_group_removed\n",
    "\n",
    "    state_after_pit = '#B#Entities = %s\\nGroups = %s\\nTriples = %s' % (human_format(n_relevant_entities), human_format(n_groups_after_restricted), human_format(n_triples_after_restricted-n_temp_rel_pit_deprecation_triples))\n",
    "    how_many_pit = '<R<<<u>Keep most recent value<br></br>(temporal functional relations)</u><br></br>Triples = %s (-%.1f%%)>' % (human_format(n_temp_rel_pit_deprecation_triples), n_restricted_triples/n_triples_after_restricted*100)\n",
    "\n",
    "    n_triples_after_pit = n_triples_after_restricted - n_temp_rel_pit_deprecation_triples\n",
    "    n_groups_after_pit = n_groups_after_restricted\n",
    "\n",
    "    state_after_corrections = '<B<<Entities = %s<br></br>Groups = %s<br></br>Triples = %s>' % (human_format(n_relevant_entities), human_format(n_groups_after_pit), human_format(n_triples_after_pit-n_corrections_triples))\n",
    "    how_many_corrections = '<R<<<u>Correct errors in<br></br>temporal functional relation</u><br></br>Triples = %s (-%.1f%%)>' % (human_format(n_corrections_triples), n_corrections_triples/n_triples_after_pit*100)\n",
    "\n",
    "    post_n_entities = stats['post_n_wiki_entities'] \n",
    "    post_n_triples = stats['n_triples_postproc']\n",
    "    post_n_relations = len(stats['post_relation_count'])\n",
    "    post_n_groups = sum(stats['post_relation_count'].values())\n",
    "    n_empty_entities = stats['n_empty_entities']\n",
    "\n",
    "    state_after_empty_removal = '#B#Entities = %s\\nRelations = %s\\nGroups = %s\\nTriples = %s' % (human_format(post_n_entities), human_format(post_n_relations), human_format(post_n_groups), human_format(post_n_triples))\n",
    "    how_many_empty = '<R<<<u>Remove empty relations</u><br></br>Entities = %s (-%.1f%%)>' % (human_format(n_empty_entities), n_empty_entities/n_relevant_entities*100)\n",
    "\n",
    "    # Defining the structure of the tree\n",
    "\n",
    "    tree = {\n",
    "        state0: [\n",
    "            {\n",
    "                state_banned_deprecated : [\n",
    "                    {state_relevant_wikipedia : [\n",
    "                        {state_after_meta_removal : [\n",
    "                            {state_after_special_value : [\n",
    "                                {state_after_restricted_removal : [\n",
    "                                    {state_after_pit : [{state_after_corrections : [state_after_empty_removal, how_many_empty]}, \n",
    "                                                        how_many_corrections]},\n",
    "                                    how_many_pit\n",
    "                                ]},\n",
    "                                how_many_restricted\n",
    "                            ]},\n",
    "                            how_many_special\n",
    "                        ]},\n",
    "                        how_many_meta\n",
    "                    ]},\n",
    "                    how_many_irrelevant_wikipedia\n",
    "                ]},\n",
    "            how_many_banned_deprecated\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Plotting the tree\n",
    "    def transform_node(node):\n",
    "        name = node.get_label()\n",
    "        color = name[1]\n",
    "        name = name[3:]\n",
    "        node.set_label(name)\n",
    "        if color == 'R':\n",
    "            node.set_color('#e74c3c')\n",
    "            node.set_fillcolor(\"#f9c4be\")\n",
    "\n",
    "    weight_order = [5,5,5,5,5,5,5,5] + [0]*50\n",
    "    graph = tree_plot(tree, transform_node, weight_order)\n",
    "\n",
    "    graph.write('filtering_process_%s.png' % version, format='png')\n",
    "\n",
    "generate_filtering_process_image('old')\n",
    "generate_filtering_process_image('new')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proportion of new/both/old in entities/groups/triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build.utils.wd import get_info_wikidata\n",
    "import json\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "j_dict = json.load(open(osp.join(STORAGE_FOLDER, 'resources/script_stats/compute_diff.json')))\n",
    "\n",
    "y = j_dict['n_new_entities'], j_dict['n_old_entities'], j_dict['n_both_entities'], j_dict['n_new_group'], j_dict['n_old_group'], j_dict['n_both_group'], j_dict['n_new_triples'], j_dict['n_old_triples'], j_dict['n_both_triples']\n",
    "color = ['New', 'Old', 'Both']\n",
    "x = ['Entities', 'Groups', 'Triples']\n",
    "fig = make_subplots(rows=1, cols=3,\n",
    "                    subplot_titles=['Entities', 'Groups', 'Triples']\n",
    "                        )\n",
    "\n",
    "for i in range(0,len(y),3):\n",
    "    fig.add_trace(go.Bar(x=color,y=y[i:i+3],name=x[i//3]), row=1, col=i//3+1)\n",
    "\n",
    "fig.data[0].marker.color = px.colors.qualitative.Plotly[:3]\n",
    "fig.data[1].marker.color = px.colors.qualitative.Plotly[:3]\n",
    "fig.data[2].marker.color = px.colors.qualitative.Plotly[:3]\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_layout(\n",
    "    margin=dict(l=5, r=5, t=30, b=5),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image('./wikidata_diff_novelty.png', scale=4, height=150, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proportion of physically new entities in new entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "j_dict = json.load(open(osp.join(STORAGE_FOLDER,'resources/script_stats/compute_diff.json')))\n",
    "x, y = zip(*j_dict['physicality_count'].items())\n",
    "rename = {\n",
    "    'new' : 'Novel',\n",
    "    'old' : 'Not novel',\n",
    "    'unk' : 'Unknown'\n",
    "}\n",
    "x = [rename[i] for i in x]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=x,y=y))\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_layout(\n",
    "    margin=dict(l=5, r=5, t=5, b=5),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image('./physicality_stats.png', scale=4, height=100, width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datatypes before and after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build.utils.wd import get_info_wikidata\n",
    "import json\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "def get_stats(version : str, post : bool = False, top_k=5):\n",
    "    post_str = '' if not post else 'post_'\n",
    "    if not post:\n",
    "        j_dict = json.load(open(osp.join(STORAGE_FOLDER, 'resources/script_stats/process_json_dump_wikidata_%s_json.json' % version)))\n",
    "        n_triples = j_dict['n_triples']\n",
    "        datatype_count = list((x[11:],y) for x,y in j_dict.items() if x.startswith('datatype_'))\n",
    "    else:\n",
    "        j_dict = json.load(open(osp.join(STORAGE_FOLDER, 'resources/script_stats/preprocess_dump_%s.json' % version)))\n",
    "        n_triples = j_dict['n_triples_postproc']\n",
    "        datatype_count = list(j_dict['%sdatatype_count' % post_str].items())\n",
    "    \n",
    "    top_k_datatype_count = sorted(datatype_count, key=lambda x : x[1], reverse=True)\n",
    "    others_count = sum(x[1] for x in top_k_datatype_count[top_k:])\n",
    "    top_k_datatype_count = top_k_datatype_count[:top_k]\n",
    "    top_k_datatype_count.append(('others', others_count))\n",
    "    stats = [(x, y) for x,y in top_k_datatype_count]\n",
    "    # stats = {k : v/len(types) for k,v in Counter(types).items()}\n",
    "    stats = pd.DataFrame(data=stats, columns=['Data type', 'Count'])\n",
    "    return stats, n_triples\n",
    "\n",
    "top_k = 6\n",
    "new_stats, pre_n_triples_new = get_stats('new')\n",
    "old_stats, pre_n_triples_old = get_stats('old')\n",
    "post_new_stats, post_n_triples_new = get_stats('new', post=True)\n",
    "post_old_stats, post_n_triples_old = get_stats('old', post=True)\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}], [{\"type\": \"pie\"}, {\"type\": \"pie\"}]])\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(values=old_stats['Count'], labels=old_stats['Data type'], title='Old Wikidata before preprocessing<br>(%s triples)' % human_format(pre_n_triples_old), \n",
    "           domain=dict(x=[0, 0.5], y=[0, 0.5]), name='haha'),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Pie(values=post_old_stats['Count'], labels=post_old_stats['Data type'], title='Old Wikidata after preprocessing<br>(%s triples)' % human_format(post_n_triples_old),\n",
    "           domain=dict(x=[0.5, 1], y=[0, 0.5])),\n",
    "    row=1, col=2\n",
    "    \n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Pie(values=new_stats['Count'], labels=new_stats['Data type'], title='New Wikidata before preprocessing<br>(%s triples)' % human_format(pre_n_triples_new), \n",
    "           domain=dict(x=[0, 0.5], y=[0.5, 1])),\n",
    "    row=2, col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Pie(values=post_new_stats['Count'], labels=post_new_stats['Data type'], title='New Wikidata after preprocessing<br>(%s triples)' % human_format(post_n_triples_new), \n",
    "           domain=dict(x=[0.5, 1], y=[0.5, 1])),\n",
    "    row=2, col=2,\n",
    ")\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image('./wikidata_datatype_distribution.png', scale=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WFD Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build.verbalize_wikifactdiff.verbalize_wikifactdiff import SAVE_PATH\n",
    "import json\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "os.makedirs('./wfd_analysis', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [json.loads(x) for x in open(osp.join(STORAGE_FOLDER, \"wikifactdiff.jsonl\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General relation distribution\n",
    "relations = [x['relation']['label'] for x in dataset]\n",
    "c = Counter(relations)\n",
    "top_relations = [x[0] for x in c.most_common(20)]\n",
    "relations = [x if x in top_relations else 'other' for x in relations]\n",
    "relations.sort(key=lambda x : c[x], reverse=True)\n",
    "\n",
    "fig = go.Figure(data=[go.Histogram(x=relations)])\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image('./wfd_analysis/general_relation_distribution.png', scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physically new entities relation distribution\n",
    "relations = [x['relation']['label'] for x in dataset if x['subject_is_ph_new']]\n",
    "c = Counter(relations)\n",
    "top_relations = [x[0] for x in c.most_common(20)]\n",
    "relations = [x if x in top_relations else 'other' for x in relations]\n",
    "relations.sort(key=lambda x : c[x], reverse=True)\n",
    "\n",
    "fig = go.Figure(data=[go.Histogram(x=relations)])\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image('./wfd_analysis/ph_new_relation_distribution.png', scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physically old entities relation distribution\n",
    "relations = [x['relation']['label'] for x in dataset if not x['subject_is_ph_new']]\n",
    "c = Counter(relations)\n",
    "top_relations = [x[0] for x in c.most_common(20)]\n",
    "relations = [x if x in top_relations else 'other' for x in relations]\n",
    "relations.sort(key=lambda x : c[x], reverse=True)\n",
    "\n",
    "fig = go.Figure(data=[go.Histogram(x=relations)])\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image('./wfd_analysis/ph_old_relation_distribution.png', scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacement update relation distribution\n",
    "def is_replace(x):\n",
    "    objects = x['objects']\n",
    "    count = Counter(x['decision'] for x in objects)\n",
    "    return len(count) == 2 and count.get('learn') == 1 and count.get('forget') == 1\n",
    "relations = [x['relation']['label'] for x in dataset if is_replace(x)]\n",
    "c = Counter(relations)\n",
    "top_relations = [x[0] for x in c.most_common(20)]\n",
    "relations = [x if x in top_relations else 'other' for x in relations]\n",
    "relations.sort(key=lambda x : c[x], reverse=True)\n",
    "\n",
    "fig = go.Figure(data=[go.Histogram(x=relations)])\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image('./wfd_analysis/replacement_relation_distribution.png', scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update scenario distribution\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def format_scenario(counts: dict):\n",
    "    s = \"\"\n",
    "    for k, v in sorted(counts.items(), key=lambda x : x[0]):\n",
    "        s += '_'.join([k for _ in range(v)]) + \"_\"\n",
    "    s = s.rstrip('_')\n",
    "    return s\n",
    "            \n",
    "\n",
    "count_rels = defaultdict(lambda : defaultdict(lambda : 0))\n",
    "\n",
    "scenarios = [frozenset(Counter([y['decision'] for y in x['objects']]).items()) for x in dataset if not x['subject_is_ph_new']]\n",
    "c = Counter(scenarios)\n",
    "top_scenarios = [x[0] for x in c.most_common(20)]\n",
    "scenarios = ['_'.join(sorted([y[0] for y in x])) if x in top_scenarios else 'other' for x in scenarios]\n",
    "def gen():\n",
    "    for x in dataset:\n",
    "        if x['subject_is_ph_new']:\n",
    "            continue\n",
    "        yield x\n",
    "\n",
    "for i,x in enumerate(gen()) :\n",
    "    count_rels[scenarios[i]][x['relation']['label']] += 1\n",
    "for k,v in count_rels.copy().items():\n",
    "    mp = [x[0] for x in sorted(v.items(), key=lambda y : y[1], reverse=True)][:4]\n",
    "    count_rels[k] = mp\n",
    "color = []\n",
    "for i,x in enumerate(gen()):\n",
    "    s = scenarios[i]\n",
    "    v = count_rels.get(s)\n",
    "    r = x['relation']['label']\n",
    "    if r in v:\n",
    "        color.append(r)\n",
    "    else:\n",
    "        color.append('other')\n",
    "\n",
    "scenarios.sort(key=lambda x : c[x], reverse=True)\n",
    "\n",
    "fig = px.histogram(pd.DataFrame(data = {\n",
    "    'Scenario' : scenarios,\n",
    "    'Relation' : color\n",
    "}), x = 'Scenario', color='Relation')\n",
    "fig.show()\n",
    "fig.write_image('./wfd_analysis/scenario_distribution.png', scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General datatype distribution\n",
    "dt2cat = {\n",
    "    'Date' : 'Date',\n",
    "    'Entity' : 'Entity',\n",
    "    'Year' : 'Date',\n",
    "    'Month' : 'Date',\n",
    "    'String' : 'String',\n",
    "    'Quantity' : 'Quantity'\n",
    "}\n",
    "objects = [x['objects'][0] for x in dataset]\n",
    "datatypes = [dt2cat[x['description']] if 'id' not in x else 'Entity' for x in objects]\n",
    "c = Counter(datatypes)\n",
    "top_datatypes = [x[0] for x in c.most_common(20)]\n",
    "datatypes = [x if x in top_datatypes else 'other' for x in datatypes]\n",
    "datatypes.sort(key=lambda x : c[x], reverse=True)\n",
    "\n",
    "fig = go.Figure(data=[go.Histogram(x=datatypes)])\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image('./wfd_analysis/general_datatype_distribution.png', scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physically new entities datatype distribution\n",
    "dt2cat = {\n",
    "    'Date' : 'Date',\n",
    "    'Entity' : 'Entity',\n",
    "    'Year' : 'Date',\n",
    "    'Month' : 'Date',\n",
    "    'String' : 'String',\n",
    "    'Quantity' : 'Quantity'\n",
    "}\n",
    "objects = [x['objects'][0] for x in dataset if x['subject_is_ph_new']]\n",
    "datatypes = [dt2cat[x['description']] if 'id' not in x else 'Entity' for x in objects]\n",
    "c = Counter(datatypes)\n",
    "top_datatypes = [x[0] for x in c.most_common(20)]\n",
    "datatypes = [x if x in top_datatypes else 'other' for x in datatypes]\n",
    "datatypes.sort(key=lambda x : c[x], reverse=True)\n",
    "\n",
    "fig = go.Figure(data=[go.Histogram(x=datatypes)])\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image('./wfd_analysis/ph_new_datatype_distribution.png', scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physically old entities datatype distribution\n",
    "dt2cat = {\n",
    "    'Date' : 'Date',\n",
    "    'Entity' : 'Entity',\n",
    "    'Year' : 'Date',\n",
    "    'Month' : 'Date',\n",
    "    'String' : 'String',\n",
    "    'Quantity' : 'Quantity'\n",
    "}\n",
    "objects = [x['objects'][0] for x in dataset if not x['subject_is_ph_new']]\n",
    "datatypes = [dt2cat[x['description']] if 'id' not in x else 'Entity' for x in objects]\n",
    "c = Counter(datatypes)\n",
    "top_datatypes = [x[0] for x in c.most_common(20)]\n",
    "datatypes = [x if x in top_datatypes else 'other' for x in datatypes]\n",
    "datatypes.sort(key=lambda x : c[x], reverse=True)\n",
    "\n",
    "fig = go.Figure(data=[go.Histogram(x=datatypes)])\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image('./wfd_analysis/ph_old_datatype_distribution.png', scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacement datatype distribution\n",
    "dt2cat = {\n",
    "    'Date' : 'Date',\n",
    "    'Entity' : 'Entity',\n",
    "    'Year' : 'Date',\n",
    "    'Month' : 'Date',\n",
    "    'String' : 'String',\n",
    "    'Quantity' : 'Quantity'\n",
    "}\n",
    "def is_replace(x):\n",
    "    objects = x['objects']\n",
    "    count = Counter(x['decision'] for x in objects)\n",
    "    return len(count) == 2 and count.get('learn') == 1 and count.get('forget') == 1\n",
    "objects = [x['objects'][0] for x in dataset if is_replace(x)]\n",
    "datatypes = [dt2cat[x['description']] if 'id' not in x else 'Entity' for x in objects]\n",
    "c = Counter(datatypes)\n",
    "top_datatypes = [x[0] for x in c.most_common(20)]\n",
    "datatypes = [x if x in top_datatypes else 'other' for x in datatypes]\n",
    "datatypes.sort(key=lambda x : c[x], reverse=True)\n",
    "\n",
    "fig = go.Figure(data=[go.Histogram(x=datatypes)])\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image('./wfd_analysis/replacement_datatype_distribution.png', scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacement datatype distribution with 2k 'population'\n",
    "import random\n",
    "dt2cat = {\n",
    "    'Date' : 'Date',\n",
    "    'Entity' : 'Entity',\n",
    "    'Year' : 'Date',\n",
    "    'Month' : 'Date',\n",
    "    'String' : 'String',\n",
    "    'Quantity' : 'Quantity'\n",
    "}\n",
    "def is_replace(x):\n",
    "    objects = x['objects']\n",
    "    count = Counter(x['decision'] for x in objects)\n",
    "    return len(count) == 2 and count.get('learn') == 1 and count.get('forget') == 1\n",
    "subset = [x for x in dataset if is_replace(x)]\n",
    "# Use random to undersample the relation population by a factor of 14 randomly\n",
    "objects = [x['objects'][0] for x in subset if x['relation']['label'] != 'population' or random.random() < 1/14]\n",
    "datatypes = [dt2cat[x['description']] if 'id' not in x else 'Entity' for x in objects]\n",
    "c = Counter(datatypes)\n",
    "top_datatypes = [x[0] for x in c.most_common(20)]\n",
    "datatypes = [x if x in top_datatypes else 'other' for x in datatypes]\n",
    "datatypes.sort(key=lambda x : c[x], reverse=True)\n",
    "\n",
    "fig = go.Figure(data=[go.Histogram(x=datatypes)])\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image('./wfd_analysis/replacement_undersampled_datatype_distribution.png', scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacement update relation distribution\n",
    "def is_replace(x):\n",
    "    objects = x['objects']\n",
    "    count = Counter(x['decision'] for x in objects)\n",
    "    return len(count) == 2 and count.get('learn') == 1 and count.get('forget') == 1\n",
    "relations = [x['relation']['label'] for x in subset if dataset]\n",
    "relations = [x for x in relations if random.random() < 1/14 or x != 'population']\n",
    "\n",
    "c = Counter(relations)\n",
    "top_relations = [x[0] for x in c.most_common(20)]\n",
    "relations = [x if x in top_relations else 'other' for x in relations]\n",
    "relations.sort(key=lambda x : c[x], reverse=True)\n",
    "\n",
    "fig = go.Figure(data=[go.Histogram(x=relations)])\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image('./wfd_analysis/undersampled_replacement_relation_distribution.png', scale=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
